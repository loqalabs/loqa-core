runtime_name: loqa-runtime
environment: development
http:
  bind: 0.0.0.0
  port: 8080
telemetry:
  log_level: info
  otlp_endpoint: ""
  otlp_insecure: true
  prometheus_bind: ":9091"
bus:
  # Embedded NATS server for zero-dependency deployment
  embedded: true          # Set to false to use external NATS server
  port: 4222             # Port for embedded server (only used when embedded=true)

  # External NATS configuration (only used when embedded=false)
  servers:
    - nats://localhost:4222
  username: ""
  password: ""
  token: ""
  tls_insecure: false
  connect_timeout_ms: 2000
node:
  id: loqa-node-1
  role: runtime
  heartbeat_interval_ms: 2000
  heartbeat_timeout_ms: 6000
  capabilities:
    - name: runtime.core
      tier: balanced
      attributes:
        modality: orchestration
skills:
  enabled: true
  directory: ./skills
  max_concurrency: 4
  audit_privacy_scope: internal
event_store:
  path: ./data/loqa-events.db
  retention_mode: session
  retention_days: 30
  max_sessions: 10000
  vacuum_on_start: false
stt:
  enabled: false
  mode: mock
  command: "python3 stt/whisper_wrapper.py"
  model_path: base.en
  language: en
  sample_rate: 16000
  channels: 1
  frame_duration_ms: 20
  partial_every_ms: 800
  publish_interim: false
llm:
  enabled: false
  mode: mock
  endpoint: http://localhost:11434
  command: "ollama run llama3.2"
  model_fast: llama3.2:latest
  model_balanced: llama3.2:latest
  default_tier: balanced
  max_tokens: 256
  temperature: 0.7
tts:
  enabled: false
  mode: mock
  command: "python3 tts/kokoro_stub.py"
  voice: en-US
  sample_rate: 22050
  channels: 1
  chunk_duration_ms: 400
router:
  enabled: true
  default_tier: balanced
  default_voice: en-US
  target: default
